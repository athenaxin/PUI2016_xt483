{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/anaconda/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6c3ddc60e73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 272\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/anaconda/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "import numpy as np\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext()\n",
    "\n",
    "\n",
    "#Extract Bike\n",
    "    lines = sc.textFile('citibike.csv', use_unicode=False).cache()\n",
    "\n",
    "    def extractBike(partId, parts):\n",
    "        if partId == 0:\n",
    "            next(parts)\n",
    "        import csv\n",
    "        reader = csv.reader(parts)\n",
    "    \n",
    "    \n",
    "        for row in reader:\n",
    "        \n",
    "            if row[6] == 'Greenwich Ave & 8 Ave':\n",
    "                temp = row[3][:-3].split(\" \")[1].split(':')\n",
    "                if row[3].split(\" \")[0].split('-')[2] == '01':\n",
    "                    (bid, starttime, s, lat, lon) = (row[0], row[3][:-3], float(temp[0])*3600 + float(temp[1])*60 + float(temp[2]), row[7], row[8])\n",
    "                    yield (bid, starttime, s,lat, lon)\n",
    "\n",
    "    bike = lines.mapPartitionsWithIndex(extractBike)\n",
    "\n",
    "#Extract Taxi\n",
    "\n",
    "#GAlatlon = (40.73901691, -74.0026376)\n",
    "\n",
    "    lines = sc.textFile(\"yellow.csv.gz\", use_unicode=False).cache()\n",
    "    GAlatlon = (40.73901691, -74.0026376)\n",
    "\n",
    "\n",
    "    import math\n",
    "    def distance(origin, destination):\n",
    "        lat1, lon1 = origin\n",
    "        lat2, lon2 = destination\n",
    "        radius = 3959 # km\n",
    "\n",
    "        dlat = math.radians(lat2-lat1)\n",
    "        dlon = math.radians(lon2-lon1)\n",
    "        a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "            * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "        d = radius * c\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extractTaxi(partId, records):\n",
    "        if partId == 0:\n",
    "            records.next()\n",
    "        import csv\n",
    "        reader = csv.reader(records)\n",
    "        for row in reader:\n",
    "            if row[4] == \"NULL\" or row[5] == \"NULL\":\n",
    "                continue\n",
    "            origin = (float(row[4]), float(row[5]))\n",
    "            if distance(origin, GAlatlon) <= 0.25:\n",
    "                yield (row[1][:-2],row[2])\n",
    "            \n",
    "\n",
    "\n",
    "    taxi= lines.mapPartitionsWithIndex(extractTaxi)\n",
    "\n",
    "#taxi.collect()\n",
    "\n",
    "#compare time\n",
    "    import datetime\n",
    "    for i in taxi.collect():\n",
    "        a = datetime.datetime.strptime(i[0], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    for i in bike.collect():\n",
    "        b = datetime.datetime.strptime(i[1], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "\n",
    "\n",
    "    taxi = taxi.map(lambda x : datetime.datetime.strptime(x[0], '%Y-%m-%d %H:%M:%S')).collect()\n",
    "\n",
    "    bike = bike.map(lambda x : datetime.datetime.strptime(x[1], '%Y-%m-%d %H:%M:%S')).collect()\n",
    "\n",
    "    s=0\n",
    "\n",
    "    for i in taxi:\n",
    "        for t in bike:\n",
    "            if t > i:\n",
    "                temp = t - i\n",
    "                if temp.seconds < 600:\n",
    "                    s += 1\n",
    "                \n",
    "    print s\n",
    "    \n",
    "sc.stop()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10345c510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import pandas as pd\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9adf3aad4544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'taxi' is not defined"
     ]
    }
   ],
   "source": [
    "tdata = sqlContext(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
