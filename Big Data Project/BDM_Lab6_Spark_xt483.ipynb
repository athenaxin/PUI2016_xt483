{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x100775590>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're creating two RDD, one is from the README file of Spark\n",
    "# and the other is directly from a list within the notebook.\n",
    "#\n",
    "# If you downloaded Spark, the README file is in the same folder\n",
    "# as the one you extracted. If you use other package management\n",
    "# methods like 'brew', 'dnf', 'apt', etc. you will need to figure\n",
    "# out the path of Spark by printing sys.path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#'/Users/hvo/sw/spark/README.md'\n",
    "rdd = sc.textFile('/Users/sugar/Downloads/spark-2.1.0-bin-hadoop2.7/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testRdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'#', u'Apache', u'Spark'], [], [u'Spark', u'is', u'a', u'fast', u'and', u'general', u'cluster', u'computing', u'system', u'for', u'Big', u'Data.', u'It', u'provides']]\n",
      "\n",
      "[u'#', u'Apache', u'Spark']\n"
     ]
    }
   ],
   "source": [
    "# Here, we're showing the difference between map() and flatMap()\n",
    "# doing the line split and get back the first 3 elements, take(3).\n",
    "# - map() is a one-to-one mapping, just like Python, so the first\n",
    "# line prints out 3 lists, each consists words per each.\n",
    "# - flatMap() is a one-to-many mapping, like MapReduce's map(). So\n",
    "# the second line prints out only 3 words.\n",
    "\n",
    "wordsPerLine = rdd.map(lambda line: line.split()).take(3)\n",
    "words = rdd.flatMap(lambda line: line.split()).take(3)\n",
    "\n",
    "print ( '%s\\n\\n%s' %(wordsPerLine, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1), (u'alternatively,', 1), (u'\"local\"', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the word count example with Spark using the approach\n",
    "# shown in the slides, i.e. staying true to the MapReduce paradigm.\n",
    "# Note that groupByKey() will sort and group everything together by\n",
    "# keys first. Then the function in mapValues() will each get applied\n",
    "# per each (key, list of values) pair. This could be an issue if we\n",
    "# have a pair with lots of values since all of the values have to be\n",
    "# stored in memory.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda values: sum(values))\n",
    "wc.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1), (u'alternatively,', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is another approach with reduceByKey() instead of groupByKey().\n",
    "# The reduce function provided for reduceByKey() only takes 2 params\n",
    "# at a time, thus, doesn't suffer the scalability issue. It also has\n",
    "# better benefits in term of parallelism.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .reduceByKey(lambda x,y: x+y)\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 25), (u'to', 19), (u'spark', 16)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we'd like to compute the top 3 most popular words in Spark. We\n",
    "# can use the RDD's top() function directly. This is much easier\n",
    "# than the two-step MapReduce job, where we had to first compute the\n",
    "# top 3 words per partition, then another top 3 on top of that. In\n",
    "# fact, this is exactly how Spark RDD's top() function is implemented.\n",
    "# More info can be found here:\n",
    "# https://github.com/apache/spark/blob/master/python/pyspark/rdd.py#L1249\n",
    "wc.top(3, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 6 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAT_FN = 'SAT_Results.csv'\n",
    "HSD_FN = 'DOE_High_School_Directory_2014-2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Below is a way to read CSV file from within Spark directly into a \n",
    "# Spark's DataFrame, which we will not be covering yet. Just putting\n",
    "# it here so that we have a reference for now. Note that, the \n",
    "# 'parserLib' option is important for reading multi-line fields of CSV.\n",
    "df = spark.read \\\n",
    "            .format(\"com.databricks.spark.csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"parserLib\", \"UNIVOCITY\") \\\n",
    "            .load(HSD_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'DBN'),\n",
       " (1, 'SCHOOL NAME'),\n",
       " (2, 'Num of SAT Test Takers'),\n",
       " (3, 'SAT Critical Reading Avg. Score'),\n",
       " (4, 'SAT Math Avg. Score'),\n",
       " (5, 'SAT Writing Avg. Score')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We read the SAT score to our RDD. Note that the use_unicode can be\n",
    "# changed accordingly to your data file to handle Unicode. If you cannot\n",
    "# parse your data due to an 'utf8' or 'ascii' decoding issue, it might\n",
    "# be a good thing to try flipping the use_unicode parameter here.\n",
    "\n",
    "sat = sc.textFile(SAT_FN, use_unicode=False).cache()\n",
    "\n",
    "# This line for us to list the column index and column names to see\n",
    "# which column we need to use for our task. In this case, we're\n",
    "# interested in the number of test takers (#2) and the math score (#4).\n",
    "list(enumerate(sat.first().split(',')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN,SCHOOL NAME,Num of SAT Test Takers,SAT Critical Reading Avg. Score,SAT Math Avg. Score,SAT Writing Avg. Score\n",
      "02M047,47 THE AMERICAN SIGN LANGUAGE AND ENGLISH SECONDARY SCHOOL,16,395,400,387\n"
     ]
    }
   ],
   "source": [
    "# Note that, our data input includes a header line that we don't want to\n",
    "# use in analysis. We can remove the header line from our RDD by doing\n",
    "# a 'filter' to remove all rows that matches the header like below. Though\n",
    "# this works, it means that we have to apply the filter function on *all*\n",
    "# row, which could be a lot of computation.\n",
    "\n",
    "noHeaderRDD = sat.filter(lambda x: not x.startswith('DBN,SCHOOL'))\n",
    "print (sat.first())\n",
    "print (noHeaderRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('02M047', (6400, 16)),\n",
       " ('21K410', (207575, 475)),\n",
       " ('30Q301', (43120, 98)),\n",
       " ('17K382', (22066, 59)),\n",
       " ('18K637', (13335, 35))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can perform the header checking per-partition, instead\n",
    "# of per-row like below. mapPartitions() is another type of map operators\n",
    "# in Spark that is similar to Hadoop Streaming's map(). It is many-to-many.\n",
    "# RDD in Spark are divided into partitions (as we read or as provided by\n",
    "# HDFS), each partition can be processed in parallel using a function\n",
    "# supplied to the mapPartitions() call.\n",
    "# \n",
    "# In addition to mapPartitions(), Spark also provides a variation called\n",
    "# mapPartitionsWithIndex() that provides information on which partition\n",
    "# we are currently processing. Indeed, mapPartitionsWithIndex() is the\n",
    "# the operator with the lowest overhead (since mapPartitions() get mapped\n",
    "# to mapPartitionsWithIndex) and also the most efficient one among all the\n",
    "# map operators.\n",
    "#\n",
    "# So our logic below is to use the partition index to check if we're hitting\n",
    "# the header (aka the first partition). If so, we just skip the first row.\n",
    "\n",
    "def extractScores(partId, records):\n",
    "    if partId==0:\n",
    "        records.next()\n",
    "    import csv\n",
    "    reader = csv.reader(records)\n",
    "    for row in reader:\n",
    "        if row[2]!='s': # to filter our bad-quality data\n",
    "            (dbn,takers,score) = (row[0], int(row[2]), int(row[4]))\n",
    "            yield (dbn, (score*takers, takers))\n",
    "\n",
    "satScores = sat.mapPartitionsWithIndex(extractScores)\n",
    "satScores.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'dbn'),\n",
       " (1, 'school_name'),\n",
       " (2, 'boro'),\n",
       " (3, 'building_code'),\n",
       " (4, 'phone_number'),\n",
       " (5, 'fax_number'),\n",
       " (6, 'grade_span_min'),\n",
       " (7, 'grade_span_max'),\n",
       " (8, 'expgrade_span_min'),\n",
       " (9, 'expgrade_span_max'),\n",
       " (10, 'bus'),\n",
       " (11, 'subway'),\n",
       " (12, 'primary_address_line_1'),\n",
       " (13, 'city'),\n",
       " (14, 'state_code'),\n",
       " (15, 'zip'),\n",
       " (16, 'website'),\n",
       " (17, 'total_students')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the same thing with the school directory data\n",
    "schools = sc.textFile(HSD_FN, use_unicode=False).cache()\n",
    "list(enumerate(schools.first().split(',')))[:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractSchools(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        list_of_records.next() # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==58 and row[17].isdigit():\n",
    "            (dbn, boro, total_students) = (row[0], row[2], int(row[17]))\n",
    "            if total_students>500: # filter to keep the large schools\n",
    "                yield (dbn, boro)\n",
    "\n",
    "largeSchools = schools.mapPartitionsWithIndex(extractSchools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('01M450', 'Manhattan'),\n",
       " ('01M539', 'Manhattan'),\n",
       " ('01M696', 'Manhattan'),\n",
       " ('02M374', 'Manhattan'),\n",
       " ('02M400', 'Manhattan'),\n",
       " ('02M408', 'Manhattan'),\n",
       " ('02M412', 'Manhattan'),\n",
       " ('02M413', 'Manhattan'),\n",
       " ('02M416', 'Manhattan'),\n",
       " ('02M418', 'Manhattan')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largeSchools.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = largeSchools.join(satScores).values() \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\\\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bronx', 470),\n",
       " ('Manhattan', 514),\n",
       " ('Brooklyn', 487),\n",
       " ('Staten Island', 477),\n",
       " ('Queens', 474)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bronx', (1619364, 3444)),\n",
       " ('Manhattan', (3206992, 6228)),\n",
       " ('Brooklyn', (4544126, 9322)),\n",
       " ('Staten Island', (1406967, 2944)),\n",
       " ('Queens', (5190534, 10942))]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 6 - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Q56', 434),\n",
       " (' B42', 384),\n",
       " (' Bx29', 392),\n",
       " (' B11', 519),\n",
       " ('Bx1', 464),\n",
       " ('Bx33', 402),\n",
       " (' Bx21', 389),\n",
       " ('S57', 526),\n",
       " (' Q30', 507),\n",
       " (' M4', 442)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractBus(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        list_of_records.next() # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==58:\n",
    "            (dbn,bus) = (row[0], row[10])\n",
    "            for i in bus.split(','):\n",
    "                yield (dbn,i)\n",
    "\n",
    "buslines = schools.mapPartitionsWithIndex(extractBus)\n",
    "\n",
    "\n",
    "\n",
    "busScores = buslines.join(satScores).values()\\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\\\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\\n",
    "    .collect()\n",
    "\n",
    "busScores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Z to Elderts Lane-75th St', 371),\n",
       " ('A', 419),\n",
       " (' 3 to Eastern Parkway-Brooklyn Museum ; C to Franklin Ave ; F', 364),\n",
       " (' 3 to Hoyt St ; A', 402),\n",
       " (' 3 to Central Park North-110th St ; 6 to 103rd St', 446),\n",
       " (' C to Euclid Ave', 400),\n",
       " ('1 to 66th St - Lincoln Center ; 2', 514),\n",
       " (' 3 to 34th St - Penn Station ; A', 423),\n",
       " (' S to Franklin Ave ; G to Bedford-Nostrand', 443),\n",
       " (' 5 to Jackson Ave', 385)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSub(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        list_of_records.next() # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==58:\n",
    "            (dbn,bus) = (row[0], row[11])\n",
    "            for i in bus.split(','):\n",
    "                yield (dbn,i)\n",
    "\n",
    "subway = schools.mapPartitionsWithIndex(extractSub)\n",
    "subScores = subway.join(satScores).values()\\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\\\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\\n",
    "    .collect()\n",
    "subScores[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-127-03b8803aea7e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-127-03b8803aea7e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sat = LOAD 'SAT_Results' AS (dbn, school name, )\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sat = LOAD 'SAT_Results' AS (dbn, school name, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
